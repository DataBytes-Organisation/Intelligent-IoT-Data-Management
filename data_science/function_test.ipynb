{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6903589c",
      "metadata": {
        "id": "6903589c"
      },
      "source": [
        "# Sprint 2 Notebook\n",
        "\n",
        "This notebook is organized into three sections:\n",
        "1. **Function Testing**\n",
        "2. **Backend Integration**\n",
        "3. **ML Research**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f908fcc1",
      "metadata": {
        "id": "f908fcc1"
      },
      "source": [
        "## 1. Function Testing\n",
        "\n",
        "This section validates the data ingestion and correlation functions across all 4 datasets.\n",
        "## Overview\n",
        "This code analyzes four sensor datasets by examining raw vs. cleaned sensor data and the correlation between two sensors over time. I'll provide a structured analysis of the results for each dataset.\n",
        "**Analysis Methodology**\n",
        "\n",
        "The analysis utilizes:\n",
        "\n",
        "Time series data loading with datetime parsing\n",
        "\n",
        "Data cleaning for missing values\n",
        "\n",
        "Rolling correlation calculation with a 20-period window\n",
        "\n",
        "Visualization of raw vs. cleaned sensor values\n",
        "\n",
        "Time-series correlation visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Create plots directory if it doesn't exist\n",
        "if not os.path.exists(\"plots\"):\n",
        "    os.makedirs(\"plots\")\n",
        "\n",
        "# Utility functions\n",
        "def get_dataset(path):\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
        "        df = df.dropna(subset=['created_at'])\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{path}' was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_data(df, handle_binary=False, handle_negative=False):\n",
        "    \"\"\"\n",
        "    Clean dataset with options for handling binary and negative values\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to clean\n",
        "    - handle_binary: If True, adds small random noise to binary fields to improve correlation calculation\n",
        "    - handle_negative: If True, transforms negative values\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    # Create a copy to avoid modifying original\n",
        "    cleaned_df = df.copy()\n",
        "\n",
        "    # Handle missing values\n",
        "    cleaned_df = cleaned_df.dropna(subset=[\"field1\", \"field2\"])\n",
        "\n",
        "    # Handle binary values by adding small noise if needed\n",
        "    if handle_binary:\n",
        "        # Check if data appears binary (mostly 0s and 1s)\n",
        "        unique_values_f1 = cleaned_df['field1'].unique()\n",
        "        unique_values_f2 = cleaned_df['field2'].unique()\n",
        "\n",
        "        is_binary_f1 = set(unique_values_f1).issubset({0, 1}) or (len(unique_values_f1) <= 2)\n",
        "        is_binary_f2 = set(unique_values_f2).issubset({0, 1}) or (len(unique_values_f2) <= 2)\n",
        "\n",
        "        if is_binary_f1:\n",
        "            print(\"Field1 appears to be binary, adding small noise...\")\n",
        "            cleaned_df['field1'] = cleaned_df['field1'] + np.random.normal(0, 0.01, size=len(cleaned_df))\n",
        "\n",
        "        if is_binary_f2:\n",
        "            print(\"Field2 appears to be binary, adding small noise...\")\n",
        "            cleaned_df['field2'] = cleaned_df['field2'] + np.random.normal(0, 0.01, size=len(cleaned_df))\n",
        "\n",
        "    # Handle negative values if needed\n",
        "    if handle_negative:\n",
        "        if (cleaned_df['field1'] < 0).any():\n",
        "            print(\"Negative values found in field1, applying transformation...\")\n",
        "            # Option 1: Clip negative values to 0\n",
        "            cleaned_df['field1'] = cleaned_df['field1'].clip(lower=0)\n",
        "            # Option 2: Shift all values to make minimum 0\n",
        "            # min_val = cleaned_df['field1'].min()\n",
        "            # if min_val < 0:\n",
        "            #     cleaned_df['field1'] = cleaned_df['field1'] - min_val\n",
        "\n",
        "        if (cleaned_df['field2'] < 0).any():\n",
        "            print(\"Negative values found in field2, applying transformation...\")\n",
        "            cleaned_df['field2'] = cleaned_df['field2'].clip(lower=0)\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "# File paths\n",
        "datasets = [\n",
        "    \"1321079.csv\",\n",
        "    \"1350261.csv\",\n",
        "    \"3036461.csv\",\n",
        "    \"518150.csv\"\n",
        "]\n",
        "\n",
        "# Import numpy for adding noise\n",
        "import numpy as np\n",
        "\n",
        "# Test all datasets\n",
        "for file in datasets:\n",
        "    print(f\"\\n--- Testing {file} ---\")\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.isfile(file):\n",
        "        print(f\"Error: The file '{file}' was not found.\")\n",
        "        continue\n",
        "\n",
        "    raw_df = get_dataset(file)\n",
        "    if raw_df is None:\n",
        "        continue\n",
        "\n",
        "    # Display dataset information\n",
        "    print(f\"Dataset shape: {raw_df.shape}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(raw_df.head())\n",
        "    print(\"\\nColumn information:\")\n",
        "    print(raw_df.dtypes)\n",
        "\n",
        "    # Check for binary or negative values\n",
        "    binary_check = ((raw_df['field1'].isin([0, 1])).all() or\n",
        "                   (raw_df['field2'].isin([0, 1])).all())\n",
        "    negative_check = ((raw_df['field1'] < 0).any() or\n",
        "                     (raw_df['field2'] < 0).any())\n",
        "\n",
        "    print(f\"\\nBinary values detected: {binary_check}\")\n",
        "    print(f\"Negative values detected: {negative_check}\")\n",
        "\n",
        "    # Apply appropriate cleaning based on detected issues\n",
        "    cleaned_df = clean_data(raw_df,\n",
        "                           handle_binary=binary_check,\n",
        "                           handle_negative=negative_check)\n",
        "\n",
        "    if cleaned_df is None or cleaned_df.empty:\n",
        "        print(\"Cleaning resulted in an empty dataset, skipping further analysis.\")\n",
        "        continue\n",
        "\n",
        "    # Calculate rolling correlation with a window of 20\n",
        "    cleaned_df[\"c(field1,field2)\"] = cleaned_df[\"field1\"].rolling(20).corr(cleaned_df[\"field2\"])\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\nCleaned data statistics:\")\n",
        "    print(cleaned_df[[\"field1\", \"field2\"]].describe())\n",
        "\n",
        "    # Plot raw vs cleaned for field1\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(raw_df[\"created_at\"], raw_df[\"field1\"], label=\"Raw field1\", alpha=0.7)\n",
        "    plt.plot(cleaned_df[\"created_at\"], cleaned_df[\"field1\"], label=\"Cleaned field1\", alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.title(f\"Raw vs Cleaned Data for field1 ({file})\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Sensor Value\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"plots/{file.replace('.csv', '')}_field1_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Plot rolling correlation\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(cleaned_df[\"created_at\"], cleaned_df[\"c(field1,field2)\"], label=\"Rolling Correlation\")\n",
        "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)  # Reference line at 0\n",
        "    plt.legend()\n",
        "    plt.title(f\"Rolling Correlation between field1 and field2 ({file})\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Correlation Value\")\n",
        "    plt.ylim(-1.1, 1.1)  # Correlation ranges from -1 to 1\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"plots/{file.replace('.csv', '')}_correlation.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Analysis completed for {file}. Plots saved to 'plots' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU8rnQXllLjN",
        "outputId": "bb8d5492-f6f2-47f4-d093-6688649bc992"
      },
      "id": "CU8rnQXllLjN",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing 1321079.csv ---\n",
            "Dataset shape: (100, 6)\n",
            "\n",
            "First few rows:\n",
            "                 created_at  entry_id  field1  field2  field3  field4\n",
            "0 2021-04-13 17:46:57+00:00      4556    25.1    44.3    25.1    44.7\n",
            "1 2021-04-13 17:56:57+00:00      4557    25.0    44.1    25.1    44.6\n",
            "2 2021-04-13 18:06:57+00:00      4558    25.1    45.1    25.0    45.8\n",
            "3 2021-04-13 18:16:58+00:00      4559    25.1    44.9    24.9    45.5\n",
            "4 2021-04-13 18:26:58+00:00      4560    25.0    44.2    24.9    45.0\n",
            "\n",
            "Column information:\n",
            "created_at    datetime64[ns, UTC]\n",
            "entry_id                    int64\n",
            "field1                    float64\n",
            "field2                    float64\n",
            "field3                    float64\n",
            "field4                    float64\n",
            "dtype: object\n",
            "\n",
            "Binary values detected: False\n",
            "Negative values detected: False\n",
            "\n",
            "Cleaned data statistics:\n",
            "           field1      field2\n",
            "count  100.000000  100.000000\n",
            "mean    25.186000   44.550000\n",
            "std      0.244545    2.308242\n",
            "min     24.800000   38.500000\n",
            "25%     25.000000   43.175000\n",
            "50%     25.200000   44.300000\n",
            "75%     25.400000   46.325000\n",
            "max     25.700000   48.700000\n",
            "Analysis completed for 1321079.csv. Plots saved to 'plots' directory.\n",
            "\n",
            "--- Testing 1350261.csv ---\n",
            "Dataset shape: (100, 10)\n",
            "\n",
            "First few rows:\n",
            "                 created_at  entry_id  field1  field2  field3  field4  field5  \\\n",
            "0 2025-08-19 07:40:05+00:00    204460   481.6     3.0   21.11  982.53   59.31   \n",
            "1 2025-08-19 07:47:05+00:00    204461   500.3     3.1   21.27  982.49   58.96   \n",
            "2 2025-08-19 07:54:05+00:00    204462   502.7     8.6   21.55  982.44   57.28   \n",
            "3 2025-08-19 08:01:05+00:00    204463   500.4    10.5   21.91  982.45   56.11   \n",
            "4 2025-08-19 08:08:05+00:00    204464   503.1     9.3   22.11  982.39   55.21   \n",
            "\n",
            "   field6  field7  field8  \n",
            "0   21.00   45.05    4.64  \n",
            "1   21.18   45.20    4.67  \n",
            "2   21.40   45.35    4.77  \n",
            "3   21.70   45.93    4.84  \n",
            "4   21.89   46.01    4.83  \n",
            "\n",
            "Column information:\n",
            "created_at    datetime64[ns, UTC]\n",
            "entry_id                    int64\n",
            "field1                    float64\n",
            "field2                    float64\n",
            "field3                    float64\n",
            "field4                    float64\n",
            "field5                    float64\n",
            "field6                    float64\n",
            "field7                    float64\n",
            "field8                    float64\n",
            "dtype: object\n",
            "\n",
            "Binary values detected: False\n",
            "Negative values detected: False\n",
            "\n",
            "Cleaned data statistics:\n",
            "            field1      field2\n",
            "count   100.000000  100.000000\n",
            "mean    855.432000  275.847000\n",
            "std     227.677983  254.232995\n",
            "min     462.300000    3.000000\n",
            "25%     618.000000   36.875000\n",
            "50%     920.850000  228.050000\n",
            "75%    1003.600000  458.650000\n",
            "max    1230.300000  884.600000\n",
            "Analysis completed for 1350261.csv. Plots saved to 'plots' directory.\n",
            "\n",
            "--- Testing 3036461.csv ---\n",
            "Dataset shape: (31, 5)\n",
            "\n",
            "First few rows:\n",
            "                 created_at  entry_id  field1  field2  field3\n",
            "0 2025-08-18 10:50:33+00:00         1       1       1       1\n",
            "1 2025-08-18 10:50:51+00:00         2       1       1       1\n",
            "2 2025-08-18 10:51:08+00:00         3       0       0       0\n",
            "3 2025-08-18 10:51:26+00:00         4       1       1       1\n",
            "4 2025-08-18 10:51:44+00:00         5       0       0       0\n",
            "\n",
            "Column information:\n",
            "created_at    datetime64[ns, UTC]\n",
            "entry_id                    int64\n",
            "field1                      int64\n",
            "field2                      int64\n",
            "field3                      int64\n",
            "dtype: object\n",
            "\n",
            "Binary values detected: True\n",
            "Negative values detected: False\n",
            "Field1 appears to be binary, adding small noise...\n",
            "Field2 appears to be binary, adding small noise...\n",
            "\n",
            "Cleaned data statistics:\n",
            "          field1     field2\n",
            "count  31.000000  31.000000\n",
            "mean    0.418830   0.418766\n",
            "std     0.502622   0.501035\n",
            "min    -0.015430  -0.023461\n",
            "25%    -0.005944  -0.005182\n",
            "50%     0.009730   0.014683\n",
            "75%     0.998439   0.995058\n",
            "max     1.014377   1.019135\n",
            "Analysis completed for 3036461.csv. Plots saved to 'plots' directory.\n",
            "\n",
            "--- Testing 518150.csv ---\n",
            "Dataset shape: (100, 10)\n",
            "\n",
            "First few rows:\n",
            "                 created_at  entry_id  field1  field2  field3  field4  field5  \\\n",
            "0 2025-08-19 05:25:30+00:00    251127  -27.46   -3.79  -25.98     NaN   -0.78   \n",
            "1 2025-08-19 05:41:00+00:00    251128     NaN     NaN     NaN    3.04     NaN   \n",
            "2 2025-08-19 05:41:30+00:00    251129  -28.33   -3.77  -21.05     NaN   -0.86   \n",
            "3 2025-08-19 05:57:00+00:00    251130     NaN     NaN     NaN    4.16     NaN   \n",
            "4 2025-08-19 05:57:30+00:00    251131  -29.27   -3.67  -39.53     NaN   -0.93   \n",
            "\n",
            "     field6  field7  field8  \n",
            "0       NaN    5.32     NaN  \n",
            "1 -21930.26     NaN   -3.74  \n",
            "2       NaN    5.25     NaN  \n",
            "3 -21932.90     NaN   -3.73  \n",
            "4       NaN    5.17     NaN  \n",
            "\n",
            "Column information:\n",
            "created_at    datetime64[ns, UTC]\n",
            "entry_id                    int64\n",
            "field1                    float64\n",
            "field2                    float64\n",
            "field3                    float64\n",
            "field4                    float64\n",
            "field5                    float64\n",
            "field6                    float64\n",
            "field7                    float64\n",
            "field8                    float64\n",
            "dtype: object\n",
            "\n",
            "Binary values detected: False\n",
            "Negative values detected: True\n",
            "Negative values found in field1, applying transformation...\n",
            "Negative values found in field2, applying transformation...\n",
            "\n",
            "Cleaned data statistics:\n",
            "       field1  field2\n",
            "count    37.0    37.0\n",
            "mean      0.0     0.0\n",
            "std       0.0     0.0\n",
            "min       0.0     0.0\n",
            "25%       0.0     0.0\n",
            "50%       0.0     0.0\n",
            "75%       0.0     0.0\n",
            "max       0.0     0.0\n",
            "Analysis completed for 518150.csv. Plots saved to 'plots' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Analysis of Results\n",
        "\n",
        "In this analysis, we focus on the impact of data cleaning on sensor readings and the relationships between different data fields through rolling correlation graphs. The insights are derived from various datasets and visualizations.\n",
        "\n",
        "## 1. Rolling Correlation Analysis\n",
        "\n",
        "The rolling correlation graphs, such as those in graph 2 and graph6., illustrate how correlation values fluctuate over time. For instance, graph2 depicts a line graph showing the rolling correlation between two data fields, labeled as \"field1\" and \"field2,\" from the dataset \"1321079.csv.\" The graph indicates that the correlation values fluctuate significantly throughout the observed period, with peaks and troughs suggesting varying degrees of correlation between the two fields. Notably, sharp spikes and drops in the correlation values are particularly noticeable around specific timestamps, indicating significant changes in the relationship between the two fields during those periods.\n",
        "Similarly, graph 6 presents a rolling correlation between \"field1\" and \"field2\" from a dataset \"3036461.csv.\" The graph shows a constant correlation value of 1.00 throughout the observed time period, suggesting a perfect positive correlation between the two fields. This unusual consistency may prompt further investigation into the nature of the relationship and the underlying factors contributing to this correlation.\n",
        "\n",
        "## 2. Importance of Data Cleaning\n",
        "\n",
        "Data cleaning plays a crucial role in ensuring the reliability of sensor readings. The graphs in graph 1 and graph 6 highlight the differences between raw and cleaned data. For example, graph 1 compares raw and cleaned sensor data over a specified time period, showing that the cleaned data line appears smoother and more stable compared to the raw data line, which exhibits more erratic fluctuations. This suggests that the cleaning process effectively removed outliers or noise present in the raw data, leading to more reliable and interpretable results.\n",
        "In graph 6,  the importance of data cleaning is further emphasized, as the graph visually represents fluctuations in sensor values, providing insights into the data's integrity and processing. The cleaned data line demonstrates a more stable trend, indicating that data cleaning is essential for achieving reliable sensor readings.\n",
        "\n",
        "## 3. Visual Comparisons of Raw and Cleaned Data\n",
        "\n",
        "The visual comparisons in graph 3,  graph8,  and graph 9 further illustrate the impact of data cleaning on sensor readings. For example, graph 3 shows that the cleaned data line rises more steadily, peaking before experiencing a sharp decline, which could indicate a significant event or anomaly that was captured in the raw data but smoothed out in the cleaned version. This highlights the importance of data processing in understanding sensor outputs.\n",
        "the graph visually compares raw and cleaned sensor data, showcasing the differences in variability. The raw data shows slight fluctuations, while the cleaned data appears more stable, indicating that the cleaning process effectively reduced noise or outliers, providing a clearer picture of the sensor's performance over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "mWAmQBFiD7DW"
      },
      "id": "mWAmQBFiD7DW"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69c3O7AoD6hv"
      },
      "id": "69c3O7AoD6hv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "111f7e61",
      "metadata": {
        "id": "111f7e61"
      },
      "source": [
        "## 2. Backend Integration\n",
        "\n",
        "This section will handle saving outputs, API calls, and integration with the backend pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f583226",
      "metadata": {
        "id": "3f583226"
      },
      "outputs": [],
      "source": [
        "# Example placeholder for backend integration\n",
        "# Save cleaned dataset or correlation results\n",
        "\n",
        "output_path = \"cleaned_output.csv\"\n",
        "cleaned_df.to_csv(output_path, index=False)\n",
        "print(f\"Saved cleaned dataset to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9415ef8b",
      "metadata": {
        "id": "9415ef8b"
      },
      "source": [
        "## 3. ML Research\n",
        "\n",
        "This section will cover:\n",
        "- Exploratory Data Analysis (EDA)\n",
        "- Feature Engineering\n",
        "- Model Prototyping (e.g., regression/classification)\n",
        "- Evaluation Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc74df2",
      "metadata": {
        "id": "bbc74df2"
      },
      "outputs": [],
      "source": [
        "# Example placeholder for ML research\n",
        "# Future steps: implement EDA, train models, evaluate performance\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Quick visualization (example)\n",
        "sns.pairplot(cleaned_df[['sensor1', 'sensor2']].dropna())\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}